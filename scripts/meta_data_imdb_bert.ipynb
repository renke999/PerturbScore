{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import evaluate\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    PretrainedConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version, send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. check the range of discrete perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_avg_disc_cos_sim_1212(textfooler_tensor, timestep_0, timestep_1):\n",
    "    num_disc_examples = 0\n",
    "    layer_avg_output_disc = {}\n",
    "\n",
    "    for layer_id in range(1, 13):\n",
    "        layer_key = 'layer_' + str(layer_id)\n",
    "        layer_avg_output_disc[layer_key] = torch.tensor(0.0)\n",
    "\n",
    "\n",
    "    for textfooler_t0 in textfooler_tensor:\n",
    "        if not textfooler_t0['timestep'] == timestep_0:\n",
    "            continue\n",
    "\n",
    "        textfooler_t1 = None\n",
    "\n",
    "        for item in textfooler_tensor:\n",
    "            # 找到timestep_1的对应text_attack样本\n",
    "            if textfooler_t0['input_ids'].equal(item['input_ids']) and item['timestep'] == timestep_1:\n",
    "                textfooler_t1 = item\n",
    "                continue\n",
    "\n",
    "        # 保证找到结果（离散空间不一定找得到）\n",
    "        if textfooler_t1:\n",
    "            num_disc_examples += 1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # 6. 计算离散空间的[CLS]表示，以及计算连续和离散的相似度\n",
    "        for layer_id in range(1, 13):\n",
    "            layer_key = 'layer_' + str(layer_id)\n",
    "            # \\cos( f(x_{t+1}), f(x_t) )\n",
    "            disc_sim = F.cosine_similarity(textfooler_t1[layer_key], textfooler_t0[layer_key], dim=0)\n",
    "            # if layer_id == 12:\n",
    "            #     print(disc_sim)\n",
    "            #     print(textfooler_t1['raw_input'])\n",
    "            #     print(textfooler_t1['attacked_input'])\n",
    "            #     mynum += 1\n",
    "            #     print()\n",
    "            # \\cos( cont_sim, disc_sim )\n",
    "            layer_avg_output_disc[layer_key] += disc_sim\n",
    "\n",
    "        # averge on num_examples\n",
    "    for layer_id in range(1, 13):\n",
    "        layer_key = 'layer_' + str(layer_id)\n",
    "        layer_avg_output_disc[layer_key] = (layer_avg_output_disc[layer_key] / num_disc_examples).item()\n",
    "\n",
    "    print(\"num_disc_examples: {}\".format(num_disc_examples))\n",
    "\n",
    "    return layer_avg_output_disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10866\n"
     ]
    }
   ],
   "source": [
    "# a. imdb-bert-textfooler\n",
    "textfooler_imdb_tensor = torch.load('/remote-home/kren/exps/TextAttack/tensors/textattack_bert_imdb_1000sample_20221211.pt')\n",
    "\n",
    "# b. imdb-bert-random\n",
    "# textfooler_imdb_tensor = torch.load('/remote-home/kren/exps/TextAttack/tensors/textfooler_imdb_bert_random_1000sample_20221226.pt')\n",
    "\n",
    "print(len(textfooler_imdb_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_disc_examples: 999\n",
      "timestep_0: 0, timestep_1: 1, cos(s,s')=0.9022\n",
      "num_disc_examples: 999\n",
      "timestep_0: 0, timestep_1: 3, cos(s,s')=0.7312\n",
      "num_disc_examples: 999\n",
      "timestep_0: 0, timestep_1: 5, cos(s,s')=0.601\n",
      "num_disc_examples: 998\n",
      "timestep_0: 0, timestep_1: 8, cos(s,s')=0.4706\n",
      "num_disc_examples: 998\n",
      "timestep_0: 0, timestep_1: 10, cos(s,s')=0.4093\n",
      "num_disc_examples: 998\n",
      "timestep_0: 0, timestep_1: 13, cos(s,s')=0.3352\n",
      "num_disc_examples: 996\n",
      "timestep_0: 0, timestep_1: 15, cos(s,s')=0.2965\n",
      "num_disc_examples: 981\n",
      "timestep_0: 0, timestep_1: 20, cos(s,s')=0.2244\n",
      "num_disc_examples: 961\n",
      "timestep_0: 0, timestep_1: 25, cos(s,s')=0.1713\n",
      "num_disc_examples: 938\n",
      "timestep_0: 0, timestep_1: 30, cos(s,s')=0.1247\n"
     ]
    }
   ],
   "source": [
    "timestep_0 = 0\n",
    "for timestep_1 in [1, 3, 5, 8, 10, 13, 15, 20, 25, 30]:\n",
    "    layer_avg_output_disc_1212 = get_avg_disc_cos_sim_1212(textfooler_imdb_tensor, timestep_0=timestep_0, timestep_1=timestep_1)\n",
    "    print(\"timestep_0: {}, timestep_1: {}, cos(s,s')={}\".format\n",
    "          (timestep_0, timestep_1, round(layer_avg_output_disc_1212['layer_12'], 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. 构造样本对-IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_cont_cos_sim_1213(freelb_tensor, timestep_0=0, timestep_1=1):\n",
    "    \"\"\"\n",
    "    在连续的情况下，一定是\n",
    "        timestep_0 = 0\n",
    "        timestep_1 = 1\n",
    "    return: dict,\n",
    "        key = input_id\n",
    "        value = 12th layer [cls] cos_sim\n",
    "    \"\"\"\n",
    "    # 新建一个dict，key是input_id，value是最后一层的cos_sim\n",
    "    cont_cos_dict = {}\n",
    "\n",
    "    for freelb_t0 in freelb_tensor:\n",
    "        if not freelb_t0['timestep'] == timestep_0:\n",
    "            continue\n",
    "\n",
    "        freelb_t1 = None\n",
    "\n",
    "        for item in freelb_tensor:\n",
    "            # 找到timestep_1的对应text_attack样本\n",
    "            if freelb_t0['input_ids'].equal(item['input_ids']) and item['timestep'] == timestep_1:\n",
    "                freelb_t1 = item\n",
    "                continue\n",
    "\n",
    "        # 保证找到结果（连续空间一定能找到，离散空间不一定找得到）\n",
    "        if not freelb_t1:\n",
    "            continue\n",
    "\n",
    "        key = str(freelb_t0['input_ids'])\n",
    "        value = round(F.cosine_similarity(freelb_t0['layer_12'], freelb_t1['layer_12'], dim=0).item(), 4)\n",
    "\n",
    "        cont_cos_dict[key] = value\n",
    "\n",
    "    print(\"cont_cos_dict.len = {}\".format(len(cont_cos_dict)))\n",
    "    return cont_cos_dict\n",
    "\n",
    "\n",
    "def get_disc_cos_sim_1213(textfooler_tensor, timestep_0, timestep_1):\n",
    "    \"\"\"\n",
    "    return: d\n",
    "    disc_cos_dict: key=input_id, value=[cls] cos sim，用于对比s和x\n",
    "    diff_t_dict: key=input_id, value=textfooler_t1，用于读取s和s'\n",
    "    \"\"\"\n",
    "    disc_cos_dict = {}\n",
    "    diff_t_dict = {}\n",
    "\n",
    "    for textfooler_t0 in textfooler_tensor:\n",
    "        if not textfooler_t0['timestep'] == timestep_0:\n",
    "            continue\n",
    "\n",
    "        textfooler_t1 = None\n",
    "\n",
    "        for item in textfooler_tensor:\n",
    "            # 找到timestep_1的对应text_attack样本\n",
    "            if textfooler_t0['input_ids'].equal(item['input_ids']) and item['timestep'] == timestep_1:\n",
    "                textfooler_t1 = item\n",
    "                continue\n",
    "\n",
    "        # 保证找到结果（离散空间不一定找得到）\n",
    "        if not textfooler_t1:\n",
    "            continue\n",
    "\n",
    "        key = str(textfooler_t0['input_ids'])\n",
    "        value = round(F.cosine_similarity(textfooler_t0['layer_12'], textfooler_t1['layer_12'], dim=0).item(), 4)\n",
    "\n",
    "        disc_cos_dict[key] = value\n",
    "        diff_t_dict[key] = textfooler_t1\n",
    "\n",
    "    print(\"timestep_1 = {}, disc_cos_dict.len = {}\".format(timestep_1, len(disc_cos_dict)))\n",
    "    return disc_cos_dict, diff_t_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(textfooler_tensor) = 10866\n"
     ]
    }
   ],
   "source": [
    "# 1. 根据不同的时间步t，加载离散的数据集，并存到字典all_disc_cos_dict中，key是timestep_1，value是对应的input_id和cos_sim的dict\n",
    "# all_diff_t_dict: key=input_id, value=textfooler_t1，用于读取s和s'\n",
    "\n",
    "\n",
    "# a. imdb-bert-textfooler\n",
    "textfooler_tensor = torch.load('/remote-home/kren/exps/TextAttack/tensors/textattack_bert_imdb_1000sample_20221211.pt')\n",
    "\n",
    "# b. imdb-bert-random\n",
    "# textfooler_tensor = torch.load('/remote-home/kren/exps/TextAttack/tensors/textfooler_imdb_bert_random_1000sample_20221226.pt')\n",
    "\n",
    "print(\"len(textfooler_tensor) = {}\".format(len(textfooler_tensor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep_1 = 1, disc_cos_dict.len = 999\n",
      "timestep_1 = 3, disc_cos_dict.len = 999\n",
      "timestep_1 = 5, disc_cos_dict.len = 999\n",
      "timestep_1 = 8, disc_cos_dict.len = 998\n",
      "timestep_1 = 10, disc_cos_dict.len = 998\n",
      "timestep_1 = 13, disc_cos_dict.len = 998\n",
      "timestep_1 = 15, disc_cos_dict.len = 996\n",
      "timestep_1 = 20, disc_cos_dict.len = 981\n",
      "timestep_1 = 25, disc_cos_dict.len = 961\n",
      "timestep_1 = 30, disc_cos_dict.len = 938\n"
     ]
    }
   ],
   "source": [
    "# timestep_1_list = [1, 3, 5]\n",
    "# 985 963 945\n",
    "timestep_1_list = [1, 3, 5, 8, 10, 13, 15, 20, 25, 30]\n",
    "all_disc_cos_dict = {}\n",
    "all_diff_t_dict = {}\n",
    "for timestep_1 in timestep_1_list:\n",
    "    key = timestep_1\n",
    "    disc_cos_dict, diff_t_dict = get_disc_cos_sim_1213(textfooler_tensor, timestep_0=0, timestep_1=timestep_1)\n",
    "    all_disc_cos_dict[key] = disc_cos_dict\n",
    "    all_diff_t_dict[key] = diff_t_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5e-1\n",
      "0.6e-1\n",
      "0.7e-1\n",
      "0.8e-1\n",
      "0.9e-1\n",
      "1e-1\n",
      "1.1e-1\n",
      "1.2e-1\n",
      "1.3e-1\n",
      "1.4e-1\n",
      "1.5e-1\n",
      "1.6e-1\n",
      "1.7e-1\n",
      "1.8e-1\n",
      "1.9e-1\n",
      "2e-1\n",
      "2.1e-1\n",
      "2.2e-1\n",
      "2.3e-1\n",
      "2.4e-1\n",
      "2.5e-1\n",
      "2.6e-1\n",
      "2.7e-1\n",
      "2.8e-1\n",
      "2.9e-1\n",
      "3e-1\n",
      "3.1e-1\n",
      "3.2e-1\n",
      "3.3e-1\n",
      "3.4e-1\n",
      "3.5e-1\n",
      "3.6e-1\n",
      "3.7e-1\n",
      "3.8e-1\n",
      "3.9e-1\n",
      "4e-1\n",
      "4.1e-1\n",
      "4.2e-1\n",
      "4.3e-1\n",
      "4.4e-1\n",
      "4.5e-1\n",
      "4.6e-1\n",
      "4.7e-1\n",
      "4.8e-1\n",
      "4.9e-1\n",
      "5e-1\n",
      "5.1e-1\n",
      "5.2e-1\n",
      "5.3e-1\n",
      "5.4e-1\n",
      "5.5e-1\n",
      "5.6e-1\n",
      "5.7e-1\n",
      "5.8e-1\n",
      "5.9e-1\n",
      "6e-1\n",
      "6.1e-1\n",
      "6.2e-1\n",
      "6.3e-1\n",
      "6.4e-1\n",
      "6.5e-1\n",
      "6.6e-1\n",
      "6.7e-1\n",
      "6.8e-1\n",
      "6.9e-1\n",
      "7e-1\n",
      "7.1e-1\n",
      "7.2e-1\n",
      "7.3e-1\n",
      "7.4e-1\n",
      "7.5e-1\n",
      "7.6e-1\n",
      "7.7e-1\n",
      "7.8e-1\n",
      "7.9e-1\n",
      "8e-1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['0.5e-1',\n",
       " '0.6e-1',\n",
       " '0.7e-1',\n",
       " '0.8e-1',\n",
       " '0.9e-1',\n",
       " '1e-1',\n",
       " '1.1e-1',\n",
       " '1.2e-1',\n",
       " '1.3e-1',\n",
       " '1.4e-1',\n",
       " '1.5e-1',\n",
       " '1.6e-1',\n",
       " '1.7e-1',\n",
       " '1.8e-1',\n",
       " '1.9e-1',\n",
       " '2e-1',\n",
       " '2.1e-1',\n",
       " '2.2e-1',\n",
       " '2.3e-1',\n",
       " '2.4e-1',\n",
       " '2.5e-1',\n",
       " '2.6e-1',\n",
       " '2.7e-1',\n",
       " '2.8e-1',\n",
       " '2.9e-1',\n",
       " '3e-1',\n",
       " '3.1e-1',\n",
       " '3.2e-1',\n",
       " '3.3e-1',\n",
       " '3.4e-1',\n",
       " '3.5e-1',\n",
       " '3.6e-1',\n",
       " '3.7e-1',\n",
       " '3.8e-1',\n",
       " '3.9e-1',\n",
       " '4e-1',\n",
       " '4.1e-1',\n",
       " '4.2e-1',\n",
       " '4.3e-1',\n",
       " '4.4e-1',\n",
       " '4.5e-1',\n",
       " '4.6e-1',\n",
       " '4.7e-1',\n",
       " '4.8e-1',\n",
       " '4.9e-1',\n",
       " '5e-1',\n",
       " '5.1e-1',\n",
       " '5.2e-1',\n",
       " '5.3e-1',\n",
       " '5.4e-1',\n",
       " '5.5e-1',\n",
       " '5.6e-1',\n",
       " '5.7e-1',\n",
       " '5.8e-1',\n",
       " '5.9e-1',\n",
       " '6e-1',\n",
       " '6.1e-1',\n",
       " '6.2e-1',\n",
       " '6.3e-1',\n",
       " '6.4e-1',\n",
       " '6.5e-1',\n",
       " '6.6e-1',\n",
       " '6.7e-1',\n",
       " '6.8e-1',\n",
       " '6.9e-1',\n",
       " '7e-1',\n",
       " '7.1e-1',\n",
       " '7.2e-1',\n",
       " '7.3e-1',\n",
       " '7.4e-1',\n",
       " '7.5e-1',\n",
       " '7.6e-1',\n",
       " '7.7e-1',\n",
       " '7.8e-1',\n",
       " '7.9e-1',\n",
       " '8e-1']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. 根据不同的anorm，加载连续的数据集，并存到字典all_cont_cos_dict中，key是anorm，value是对应的input_id和cos_sim的dict\n",
    "adv_max_norm_list = []\n",
    "\n",
    "# [0.5e-1, 0.9e-1]\n",
    "for i in range(0, 1):\n",
    "    for j in range(5, 10):\n",
    "        print(\"{}.{}e-1\".format(i, j))\n",
    "        adv_max_norm_list.append(\"{}.{}e-1\".format(i, j))\n",
    "\n",
    "# [1e-1, 8e-1)\n",
    "for i in range(1, 8):\n",
    "    print(\"{}e-1\".format(i))\n",
    "    adv_max_norm_list.append(\"{}e-1\".format(i))\n",
    "    for j in range(1, 10):\n",
    "        print(\"{}.{}e-1\".format(i,j))\n",
    "        adv_max_norm_list.append(\"{}.{}e-1\".format(i,j))\n",
    "\n",
    "# [8e-1]\n",
    "for i in range(8, 9):\n",
    "    print(\"{}e-1\".format(i))\n",
    "    adv_max_norm_list.append(\"{}e-1\".format(i))\n",
    "adv_max_norm_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm0.5e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm0.6e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm0.7e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm0.8e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm0.9e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm1e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm1.1e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm1.2e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm1.3e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm1.4e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm1.5e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm1.6e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm1.7e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm1.8e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm1.9e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm2e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm2.1e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm2.2e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm2.3e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm2.4e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm2.5e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm2.6e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm2.7e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm2.8e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm2.9e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm3e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm3.1e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm3.2e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm3.3e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm3.4e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm3.5e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm3.6e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm3.7e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm3.8e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm3.9e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm4e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm4.1e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm4.2e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm4.3e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm4.4e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm4.5e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm4.6e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm4.7e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm4.8e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm4.9e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm5e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm5.1e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm5.2e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm5.3e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm5.4e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm5.5e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm5.6e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm5.7e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm5.8e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm5.9e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm6e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm6.1e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm6.2e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm6.3e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm6.4e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm6.5e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm6.6e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm6.7e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm6.8e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm6.9e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm7e-1_as15.pt\n",
      "cont_cos_dict.len = 1000\n",
      "/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/freelb_bert_imdb_alr1e-1_amag0e-0_anm7.1e-1_as15.pt\n"
     ]
    }
   ],
   "source": [
    "# adv_max_norm_list = ['1e-1', '1.1e-1']\n",
    "all_cont_cos_dict = {}\n",
    "\n",
    "for anorm in adv_max_norm_list:\n",
    "    adv_lr = '1e-1'\n",
    "    adv_init_mag = '0e-0'\n",
    "    adv_max_norm = anorm\n",
    "    adv_steps = '15'\n",
    "    root_path = '/remote-home/kren/exps/FreeLB/hf424_torch182/tensors/'\n",
    "    file_name = 'freelb_bert_imdb_alr' + str(adv_lr) + '_amag' + str(adv_init_mag) + '_anm' + str(adv_max_norm) + '_as' + str(adv_steps)\n",
    "    freelb_path = root_path + file_name + '.pt'\n",
    "    print(freelb_path)\n",
    "    freelb_tensor = torch.load(freelb_path)\n",
    "\n",
    "    key = anorm\n",
    "    value = get_cont_cos_sim_1213(freelb_tensor, timestep_0=0, timestep_1=1)\n",
    "    all_cont_cos_dict[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching timestep=1 disc perturbation\n",
      "timestep=1, no result num=107\n",
      "searching timestep=3 disc perturbation\n",
      "timestep=3, no result num=97\n",
      "searching timestep=5 disc perturbation\n",
      "timestep=5, no result num=85\n",
      "searching timestep=8 disc perturbation\n",
      "timestep=8, no result num=80\n",
      "searching timestep=10 disc perturbation\n",
      "timestep=10, no result num=73\n",
      "searching timestep=13 disc perturbation\n",
      "timestep=13, no result num=83\n",
      "searching timestep=15 disc perturbation\n",
      "timestep=15, no result num=88\n",
      "searching timestep=20 disc perturbation\n",
      "timestep=20, no result num=116\n",
      "searching timestep=25 disc perturbation\n",
      "timestep=25, no result num=115\n",
      "searching timestep=30 disc perturbation\n",
      "timestep=30, no result num=135\n",
      "searing done\n"
     ]
    }
   ],
   "source": [
    "# 3. 开始搜索，all_examples存放所有搜索到的结果\n",
    "all_examples = []\n",
    "\n",
    "# a. 遍历不同时间步下离散的扰动\n",
    "for timestep, disc_dict in all_disc_cos_dict.items():\n",
    "\n",
    "    # b. 遍历单个时间步下离散的扰动，no_find_num统计没有找到相似的数量, range_list用于保存差值范围较大的结果\n",
    "    print(\"searching timestep={} disc perturbation\".format(timestep))\n",
    "    no_find_num = 0\n",
    "    for disc_input, disc_sim in disc_dict.items():\n",
    "        find_flag = False\n",
    "        range1_list = []\n",
    "        range2_list = []\n",
    "\n",
    "        # c. 遍历连续的扰动，寻找与离散的扰动最接近的那一个\n",
    "        for epsilon, cont_dict in all_cont_cos_dict.items():\n",
    "            # d. 找到对应epsilon的连续扰动的cos_sim (cont_sim)\n",
    "            try:\n",
    "                cont_sim = cont_dict[disc_input]\n",
    "            except:\n",
    "                print(\"wrong! check code!\")\n",
    "                continue\n",
    "\n",
    "            one_example = {\"raw\": all_diff_t_dict[timestep][disc_input]['raw_input'],\n",
    "                           \"attacked\": all_diff_t_dict[timestep][disc_input]['attacked_input'],\n",
    "                           \"epsilon\": float(epsilon),\n",
    "                           \"timestep\": timestep,\n",
    "                           \"cont_sim\":cont_sim,\n",
    "                           \"disc_sim\": disc_sim,\n",
    "                           \"difference\": \"\"}\n",
    "\n",
    "            # if disc_sim <= 0.0:\n",
    "            #     # 如果cos(s, s') <= 0，则按0计算，此时对应连续扰动的epsilon=6e-1\n",
    "            #     find_flag = True\n",
    "            #     one_example['epsilon'] = 6e-1\n",
    "            #     one_example['difference'] = \"cos<0\"\n",
    "            #     all_examples.append(one_example)\n",
    "            #     break\n",
    "            #\n",
    "            # if disc_sim <= 0.0:\n",
    "            #     # 如果cos(s, s') <= 0，则按0.0考虑\n",
    "            #     disc_sim = 0.0\n",
    "\n",
    "            if abs(cont_sim - disc_sim) <= 0.005:\n",
    "                # 如果差值 <= 0.005，说明成功找到了\n",
    "                find_flag = True\n",
    "                one_example['difference'] = '0.005'\n",
    "                all_examples.append(one_example)\n",
    "                break\n",
    "            elif abs(cont_sim - disc_sim) <= 0.01:\n",
    "                # 如果没找到差值 <= 0.005，则开始找差值 <= 0.01\n",
    "                one_example['difference'] = '0.01'\n",
    "                range1_list.append(one_example)\n",
    "            elif abs(cont_sim - disc_sim) <= 0.02:\n",
    "                # 如果没找到差值 <= 0.01，则开始找差值 <= 0.02\n",
    "                one_example['difference'] = '0.02'\n",
    "                range2_list.append(one_example)\n",
    "\n",
    "        # 没有找到相差<=0.005的结果\n",
    "        if not find_flag:\n",
    "            if range1_list:\n",
    "                all_examples.append(range1_list[0])\n",
    "            elif range2_list:\n",
    "                all_examples.append(range2_list[0])\n",
    "            else:\n",
    "                no_find_num += 1\n",
    "\n",
    "    print(\"timestep={}, no result num={}\".format(timestep, no_find_num))\n",
    "\n",
    "print(\"searing done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. 构造meta_scorer输入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. 循环遍历raw_sentence和attacked_sentence\n",
    "2. 当出现不一致时：\n",
    "    1. 从这个不一致的位置想办法提取出一个词（分词）\n",
    "    2. 构建模型的输入\n",
    "    3. 继续循环遍历"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## a. 根据raw和attacked构建输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def get_changed_text(s_raw, s_att):\n",
    "    \"\"\"\n",
    "    s_raw: 原始句子\n",
    "    s_att: 攻击后的句子\n",
    "    return：word1[word2]的格式\n",
    "    \"\"\"\n",
    "    tokenized_raw = word_tokenize(s_raw)\n",
    "    tokenized_attacked = word_tokenize(s_att)\n",
    "    tidx = 0\n",
    "    sidx = 0\n",
    "    new_input = ''\n",
    "    while sidx < len(s_raw):\n",
    "        if s_raw[sidx] == ' ':\n",
    "            new_input += ' '\n",
    "            sidx += 1\n",
    "        elif (s_raw[sidx] == '\"' and tokenized_raw[tidx] == \"``\") or (s_raw[sidx] == '\"' and tokenized_raw[tidx] == \"''\"):\n",
    "            # word_tokenize会将双引号分词为两个反引号\n",
    "            # https://stackoverflow.com/questions/32185072/nltk-word-tokenize-behaviour-for-double-quotation-marks-is-confusing\n",
    "            new_input += '\"'\n",
    "            sidx += 1\n",
    "            tidx += 1\n",
    "        elif (s_raw[sidx:sidx+2] == \"''\" and tokenized_raw[tidx] == \"``\") or (s_raw[sidx:sidx+2] == \"''\" and tokenized_raw[tidx] == \"''\"):\n",
    "            new_input += \"''\"\n",
    "            sidx += 2\n",
    "            tidx += 1\n",
    "        elif s_raw[sidx] == '\\x85' or s_raw[sidx] == '\\xa0' or s_raw[sidx] == '\\t':\n",
    "            new_input += '\"'\n",
    "            sidx += 1\n",
    "        elif tokenized_raw[tidx] == tokenized_attacked[tidx]:\n",
    "            # 如果这两个词是一样的\n",
    "            # 保证结果正确\n",
    "            # print(sidx)\n",
    "            assert not s_raw[sidx] == ' '\n",
    "            if s_raw[sidx] != tokenized_raw[tidx][0]:\n",
    "                print(\"diff. idx: {}\".format(sidx))\n",
    "            assert s_raw[sidx] == tokenized_raw[tidx][0]\n",
    "            new_input += tokenized_raw[tidx]\n",
    "            sidx += len(tokenized_raw[tidx])\n",
    "            tidx += 1\n",
    "        elif tokenized_raw[tidx] != tokenized_attacked[tidx]:\n",
    "            # 如果两个词不一样，则构造输入 word1[word2]\n",
    "            # 保证结果正确\n",
    "            assert not s_raw[sidx] == ' '\n",
    "            assert s_raw[sidx] == tokenized_raw[tidx][0]\n",
    "            new_input += tokenized_raw[tidx] + '[' + tokenized_attacked[tidx] + ']'\n",
    "            sidx += len(tokenized_raw[tidx])\n",
    "            tidx += 1\n",
    "        else:\n",
    "            raise Exception('wrong')\n",
    "    return new_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8982"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "one_example = {\"raw\": all_diff_t_dict[timestep][disc_input]['raw_input'],\n",
    "               \"attacked\": all_diff_t_dict[timestep][disc_input]['attacked_input'],\n",
    "               \"epsilon\": float(epsilon),\n",
    "               \"cont_sim\":cont_sim,\n",
    "               \"disc_sim\": disc_sim,\n",
    "               \"difference\": \"\"}\n",
    "\"\"\"\n",
    "len(all_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 8982/8982 [00:39<00:00, 229.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total failed num: 70\n",
      "failed index list: [184, 1082, 1096, 1987, 2002, 2904, 2919, 3162, 3827, 3842, 4083, 4086, 4222, 4545, 4754, 4769, 4887, 5012, 5015, 5096, 5144, 5221, 5460, 5670, 5685, 5803, 5929, 5932, 6014, 6064, 6142, 6371, 6573, 6578, 6579, 6593, 6709, 6914, 6961, 7033, 7252, 7387, 7447, 7454, 7459, 7460, 7475, 7586, 7711, 7792, 7841, 7911, 8117, 8125, 8264, 8322, 8332, 8333, 8347, 8462, 8479, 8545, 8576, 8660, 8684, 8703, 8752, 8772, 8960, 8968]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "failed_num = 0\n",
    "failed_index = []\n",
    "for i in tqdm(range(len(all_examples))):\n",
    "    res = None\n",
    "    try:\n",
    "        res = get_changed_text(all_examples[i]['raw'], all_examples[i]['attacked'])\n",
    "    except:\n",
    "        failed_index.append(i)\n",
    "        res = ''\n",
    "        failed_num += 1\n",
    "    finally:\n",
    "        all_examples[i]['model_input'] = res\n",
    "print(\"total failed num: {}\".format(failed_num))\n",
    "print(\"failed index list: {}\".format(failed_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## b. 清除失败的index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(all_examples): 8982\n",
      "reversed failed index list: [8968, 8960, 8772, 8752, 8703, 8684, 8660, 8576, 8545, 8479, 8462, 8347, 8333, 8332, 8322, 8264, 8125, 8117, 7911, 7841, 7792, 7711, 7586, 7475, 7460, 7459, 7454, 7447, 7387, 7252, 7033, 6961, 6914, 6709, 6593, 6579, 6578, 6573, 6371, 6142, 6064, 6014, 5932, 5929, 5803, 5685, 5670, 5460, 5221, 5144, 5096, 5015, 5012, 4887, 4769, 4754, 4545, 4222, 4086, 4083, 3842, 3827, 3162, 2919, 2904, 2002, 1987, 1096, 1082, 184]\n"
     ]
    }
   ],
   "source": [
    "# 保证是倒序\n",
    "failed_index = sorted(failed_index, reverse=True)\n",
    "print(\"len(all_examples): {}\".format(len(all_examples)))\n",
    "print(\"reversed failed index list: {}\".format(failed_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(all_examples): 8912\n"
     ]
    }
   ],
   "source": [
    "# 倒序删除失败index\n",
    "for idx in failed_index:\n",
    "    del all_examples[idx]\n",
    "print(\"len(all_examples): {}\".format(len(all_examples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 保证构造的输入不为空\n",
    "for item in all_examples:\n",
    "    assert item['model_input'] != ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4. 计算bertscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"bert_score\", module_type: \"metric\", features: [{'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}, {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}], usage: \"\"\"\n",
       "BERTScore Metrics with the hashcode from a source against one or more references.\n",
       "\n",
       "Args:\n",
       "    predictions (list of str): Prediction/candidate sentences.\n",
       "    references (list of str or list of list of str): Reference sentences.\n",
       "    lang (str): Language of the sentences; required (e.g. 'en').\n",
       "    model_type (str): Bert specification, default using the suggested\n",
       "        model for the target language; has to specify at least one of\n",
       "        `model_type` or `lang`.\n",
       "    num_layers (int): The layer of representation to use,\n",
       "        default using the number of layers tuned on WMT16 correlation data.\n",
       "    verbose (bool): Turn on intermediate status update.\n",
       "    idf (bool or dict): Use idf weighting; can also be a precomputed idf_dict.\n",
       "    device (str): On which the contextual embedding model will be allocated on.\n",
       "        If this argument is None, the model lives on cuda:0 if cuda is available.\n",
       "    nthreads (int): Number of threads.\n",
       "    batch_size (int): Bert score processing batch size,\n",
       "        at least one of `model_type` or `lang`. `lang` needs to be\n",
       "        specified when `rescale_with_baseline` is True.\n",
       "    rescale_with_baseline (bool): Rescale bertscore with pre-computed baseline.\n",
       "    baseline_path (str): Customized baseline file.\n",
       "    use_fast_tokenizer (bool): `use_fast` parameter passed to HF tokenizer. New in version 0.3.10.\n",
       "\n",
       "Returns:\n",
       "    precision: Precision.\n",
       "    recall: Recall.\n",
       "    f1: F1 score.\n",
       "    hashcode: Hashcode of the library.\n",
       "\n",
       "Examples:\n",
       "\n",
       "    >>> predictions = [\"hello there\", \"general kenobi\"]\n",
       "    >>> references = [\"hello there\", \"general kenobi\"]\n",
       "    >>> bertscore = evaluate.load(\"bertscore\")\n",
       "    >>> results = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
       "    >>> print([round(v, 2) for v in results[\"f1\"]])\n",
       "    [1.0, 1.0]\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 8912/8912 [05:29<00:00, 27.04it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# 使用tqdm对可迭代对象进行包装，实现进度条可视化\n",
    "# 计算bertscore\n",
    "for i in tqdm(range(len(all_examples))):\n",
    "    bs = bertscore.compute(predictions=[all_examples[i]['raw']], references=[all_examples[i]['attacked']], lang=\"en\")\n",
    "    all_examples[i]['bert_score_precision'] = bs['precision'][0]\n",
    "    all_examples[i]['bert_score_recall'] = bs['recall'][0]\n",
    "    all_examples[i]['bert_score_f1'] = bs['f1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 5. 计算use_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************************************************\n",
      "initial save_path successfully!\n",
      "save_path: /remote-home/kren/exps/TextAttack/tensors/textfooler_imdb_bert_random_1000sample_20221226.pt\n",
      "**************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textattack.constraints.semantics.sentence_encoders import UniversalSentenceEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "use_encoder = UniversalSentenceEncoder(\n",
    "    threshold=0.840845057,\n",
    "    metric=\"angular\",\n",
    "    compare_against_original=False,\n",
    "    window_size=15,\n",
    "    skip_text_shorter_than_window=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_use_sim(raw_input, attacked_input):\n",
    "    raw_embedding, attacked_embedding = use_encoder.encode(\n",
    "        [raw_input, attacked_input]\n",
    "    )\n",
    "    # raw_embedding: 已由<class 'tensorflow.python.framework.ops.EagerTensor'>转换为numpy\n",
    "    if not isinstance(raw_embedding, torch.Tensor):\n",
    "        raw_embedding = torch.tensor(raw_embedding)\n",
    "\n",
    "    if not isinstance(attacked_embedding, torch.Tensor):\n",
    "        attacked_embedding = torch.tensor(attacked_embedding)\n",
    "\n",
    "    # 为了满足torch的输入要求，增加一个batch维\n",
    "    raw_embedding = torch.unsqueeze(raw_embedding, dim=0)\n",
    "    attacked_embedding = torch.unsqueeze(attacked_embedding, dim=0)\n",
    "\n",
    "    # cos_sim\n",
    "    sim_metric = torch.nn.CosineSimilarity(dim=1)\n",
    "    return sim_metric(raw_embedding, attacked_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 8912/8912 [01:24<00:00, 104.85it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# 计算use_sim\n",
    "for i in tqdm(range(len(all_examples))):\n",
    "    use_sim = get_use_sim(all_examples[i]['raw'], all_examples[i]['attacked'])\n",
    "    all_examples[i]['use_sim'] = use_sim.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 5. 保存为csv文件并进行数据分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "meta_df = pd.DataFrame(all_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw</th>\n",
       "      <th>attacked</th>\n",
       "      <th>epsilon</th>\n",
       "      <th>timestep</th>\n",
       "      <th>cont_sim</th>\n",
       "      <th>disc_sim</th>\n",
       "      <th>difference</th>\n",
       "      <th>model_input</th>\n",
       "      <th>bert_score_precision</th>\n",
       "      <th>bert_score_recall</th>\n",
       "      <th>bert_score_f1</th>\n",
       "      <th>use_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My girlfriend once brought around The Zombie C...</td>\n",
       "      <td>My girlfriend once brought around The Zombie C...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.005</td>\n",
       "      <td>My girlfriend once brought around The Zombie C...</td>\n",
       "      <td>0.997637</td>\n",
       "      <td>0.996182</td>\n",
       "      <td>0.996909</td>\n",
       "      <td>0.995793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I saw this film opening weekend in Australia, ...</td>\n",
       "      <td>I saw this film opening weekend in Australia, ...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9942</td>\n",
       "      <td>0.9975</td>\n",
       "      <td>0.005</td>\n",
       "      <td>I saw this film opening weekend in Australia, ...</td>\n",
       "      <td>0.994779</td>\n",
       "      <td>0.992669</td>\n",
       "      <td>0.993723</td>\n",
       "      <td>0.995721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I was excited to view a Cataluña´s film in the...</td>\n",
       "      <td>I was excited to view a Cataluña´s film in the...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>0.01</td>\n",
       "      <td>I was excited to view a Cataluña´s film in the...</td>\n",
       "      <td>0.991487</td>\n",
       "      <td>0.993112</td>\n",
       "      <td>0.992299</td>\n",
       "      <td>0.989435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Terrible movie. Nuff Said.&lt;br /&gt;&lt;br /&gt;These Li...</td>\n",
       "      <td>Terrible movie. Nuff Said.&lt;br /&gt;&lt;br /&gt;These Li...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>0.005</td>\n",
       "      <td>Terrible movie. Nuff Said.&lt;br /&gt;&lt;br /&gt;These Li...</td>\n",
       "      <td>0.994131</td>\n",
       "      <td>0.990593</td>\n",
       "      <td>0.992359</td>\n",
       "      <td>0.997162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I saw the capsule comment said \"great acting.\"...</td>\n",
       "      <td>I saw the capsule comment said \"great acting.\"...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>0.005</td>\n",
       "      <td>I saw the capsule comment said \"great acting.\"...</td>\n",
       "      <td>0.995719</td>\n",
       "      <td>0.994117</td>\n",
       "      <td>0.994917</td>\n",
       "      <td>0.994693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8907</th>\n",
       "      <td>\"Bread\" very sharply skewers the conventions o...</td>\n",
       "      <td>\"Fuchsia\" very indicator gratifying the franço...</td>\n",
       "      <td>0.57</td>\n",
       "      <td>30</td>\n",
       "      <td>0.7998</td>\n",
       "      <td>0.8008</td>\n",
       "      <td>0.005</td>\n",
       "      <td>\"Bread[Fuchsia]\" very sharply[indicator] skewe...</td>\n",
       "      <td>0.880684</td>\n",
       "      <td>0.827222</td>\n",
       "      <td>0.853116</td>\n",
       "      <td>0.371166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8908</th>\n",
       "      <td>I can't remember many films where a bumbling i...</td>\n",
       "      <td>snort can't rigged erratic circulates where a ...</td>\n",
       "      <td>0.35</td>\n",
       "      <td>30</td>\n",
       "      <td>-0.5608</td>\n",
       "      <td>-0.5748</td>\n",
       "      <td>0.02</td>\n",
       "      <td>I[snort] can't remember[rigged] many[erratic] ...</td>\n",
       "      <td>0.826505</td>\n",
       "      <td>0.792810</td>\n",
       "      <td>0.809307</td>\n",
       "      <td>0.293184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8909</th>\n",
       "      <td>Allen and Julie move into a cabin in the mount...</td>\n",
       "      <td>Allen and Julie eligibility into a wl in the w...</td>\n",
       "      <td>0.54</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9083</td>\n",
       "      <td>0.9034</td>\n",
       "      <td>0.005</td>\n",
       "      <td>Allen and Julie move[eligibility] into a cabin...</td>\n",
       "      <td>0.930775</td>\n",
       "      <td>0.890104</td>\n",
       "      <td>0.909985</td>\n",
       "      <td>0.778371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8910</th>\n",
       "      <td>I definitely recommend reading the book prior ...</td>\n",
       "      <td>I definitely recommend reading the book prior ...</td>\n",
       "      <td>0.26</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9927</td>\n",
       "      <td>0.9879</td>\n",
       "      <td>0.005</td>\n",
       "      <td>I definitely recommend reading the book prior ...</td>\n",
       "      <td>0.953927</td>\n",
       "      <td>0.934702</td>\n",
       "      <td>0.944217</td>\n",
       "      <td>0.932000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8911</th>\n",
       "      <td>Recap: It's business as usual at Louche's casi...</td>\n",
       "      <td>Recap: It's business as mornin at Louche's cas...</td>\n",
       "      <td>0.21</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9901</td>\n",
       "      <td>0.9859</td>\n",
       "      <td>0.005</td>\n",
       "      <td>Recap: It's business as usual[mornin] at Louch...</td>\n",
       "      <td>0.960776</td>\n",
       "      <td>0.944645</td>\n",
       "      <td>0.952642</td>\n",
       "      <td>0.930928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8912 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    raw  \\\n",
       "0     My girlfriend once brought around The Zombie C...   \n",
       "1     I saw this film opening weekend in Australia, ...   \n",
       "2     I was excited to view a Cataluña´s film in the...   \n",
       "3     Terrible movie. Nuff Said.<br /><br />These Li...   \n",
       "4     I saw the capsule comment said \"great acting.\"...   \n",
       "...                                                 ...   \n",
       "8907  \"Bread\" very sharply skewers the conventions o...   \n",
       "8908  I can't remember many films where a bumbling i...   \n",
       "8909  Allen and Julie move into a cabin in the mount...   \n",
       "8910  I definitely recommend reading the book prior ...   \n",
       "8911  Recap: It's business as usual at Louche's casi...   \n",
       "\n",
       "                                               attacked  epsilon  timestep  \\\n",
       "0     My girlfriend once brought around The Zombie C...     0.10         1   \n",
       "1     I saw this film opening weekend in Australia, ...     0.10         1   \n",
       "2     I was excited to view a Cataluña´s film in the...     0.10         1   \n",
       "3     Terrible movie. Nuff Said.<br /><br />These Li...     0.10         1   \n",
       "4     I saw the capsule comment said \"great acting.\"...     0.10         1   \n",
       "...                                                 ...      ...       ...   \n",
       "8907  \"Fuchsia\" very indicator gratifying the franço...     0.57        30   \n",
       "8908  snort can't rigged erratic circulates where a ...     0.35        30   \n",
       "8909  Allen and Julie eligibility into a wl in the w...     0.54        30   \n",
       "8910  I definitely recommend reading the book prior ...     0.26        30   \n",
       "8911  Recap: It's business as mornin at Louche's cas...     0.21        30   \n",
       "\n",
       "      cont_sim  disc_sim difference  \\\n",
       "0       0.9995    0.9999      0.005   \n",
       "1       0.9942    0.9975      0.005   \n",
       "2       0.9945    0.9995       0.01   \n",
       "3       0.9992    0.9996      0.005   \n",
       "4       0.9996    0.9996      0.005   \n",
       "...        ...       ...        ...   \n",
       "8907    0.7998    0.8008      0.005   \n",
       "8908   -0.5608   -0.5748       0.02   \n",
       "8909    0.9083    0.9034      0.005   \n",
       "8910    0.9927    0.9879      0.005   \n",
       "8911    0.9901    0.9859      0.005   \n",
       "\n",
       "                                            model_input  bert_score_precision  \\\n",
       "0     My girlfriend once brought around The Zombie C...              0.997637   \n",
       "1     I saw this film opening weekend in Australia, ...              0.994779   \n",
       "2     I was excited to view a Cataluña´s film in the...              0.991487   \n",
       "3     Terrible movie. Nuff Said.<br /><br />These Li...              0.994131   \n",
       "4     I saw the capsule comment said \"great acting.\"...              0.995719   \n",
       "...                                                 ...                   ...   \n",
       "8907  \"Bread[Fuchsia]\" very sharply[indicator] skewe...              0.880684   \n",
       "8908  I[snort] can't remember[rigged] many[erratic] ...              0.826505   \n",
       "8909  Allen and Julie move[eligibility] into a cabin...              0.930775   \n",
       "8910  I definitely recommend reading the book prior ...              0.953927   \n",
       "8911  Recap: It's business as usual[mornin] at Louch...              0.960776   \n",
       "\n",
       "      bert_score_recall  bert_score_f1   use_sim  \n",
       "0              0.996182       0.996909  0.995793  \n",
       "1              0.992669       0.993723  0.995721  \n",
       "2              0.993112       0.992299  0.989435  \n",
       "3              0.990593       0.992359  0.997162  \n",
       "4              0.994117       0.994917  0.994693  \n",
       "...                 ...            ...       ...  \n",
       "8907           0.827222       0.853116  0.371166  \n",
       "8908           0.792810       0.809307  0.293184  \n",
       "8909           0.890104       0.909985  0.778371  \n",
       "8910           0.934702       0.944217  0.932000  \n",
       "8911           0.944645       0.952642  0.930928  \n",
       "\n",
       "[8912 rows x 12 columns]"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# a. imdb-bert-textfooler\n",
    "csv_folder_path = '/remote-home/kren/exps/FreeLB/hf424_torch182/meta_data/imdb_bert_textfooler'\n",
    "\n",
    "# b. imdb-bert-random\n",
    "# csv_folder_path = '/remote-home/kren/exps/FreeLB/hf424_torch182/meta_data/imdb_bert_rand'\n",
    "\n",
    "csv_file_name = 'preprocessed_data.csv'\n",
    "csv_save_path = csv_folder_path + '/' + csv_file_name\n",
    "meta_df.to_csv(csv_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## a. < 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7878\n"
     ]
    }
   ],
   "source": [
    "print(len(meta_df[meta_df['difference'] == '0.005']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## b. 不同时间段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,0.1]: 4630\n",
      "(0.1,0.2]: 2265\n",
      "(0.2,0.3]: 1075\n",
      "(0.3,0.4]: 528\n",
      "(0.4,0.5]: 224\n",
      "(0.5,0.6]: 104\n",
      "(0.6,0.7]: 54\n",
      "(0.7,0.8]: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/remote-home/kren/anaconda3/envs/hf424_torch182/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "range_list = [(0, 0.1), (0.1, 0.2), (0.2, 0.3), (0.3, 0.4), (0.4, 0.5), (0.5, 0.6), (0.6, 0.7), (0.7, 0.8)]\n",
    "for lower, upper in range_list:\n",
    "    range_num = len(meta_df[meta_df['epsilon'] > lower][meta_df['epsilon'] <= upper])\n",
    "    print(\"({},{}]: {}\".format(lower, upper, range_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 6. 划分训练集和测试集(80% train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8912"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_all_examples = len(all_examples)\n",
    "len_all_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 9158\n",
    "eval_idx_list = [idx for idx in range(len_all_examples) if idx % 5 == 0]\n",
    "train_idx_list = [idx for idx in range(len_all_examples) if idx % 5 != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_df = meta_df.loc[train_idx_list]\n",
    "eval_df = meta_df.loc[eval_idx_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# a. imdb-bert-textfooler\n",
    "folder_path = '/remote-home/kren/exps/FreeLB/hf424_torch182/meta_data/imdb_bert_textfooler'\n",
    "\n",
    "# b. imdb-bert-random\n",
    "# folder_path = '/remote-home/kren/exps/FreeLB/hf424_torch182/meta_data/imdb_bert_rand'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "eval_df.to_csv(folder_path + '/eval_data.csv')\n",
    "train_df.to_csv(folder_path + '/train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf424_torch182",
   "language": "python",
   "name": "hf424_torch182"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
